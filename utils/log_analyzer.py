#!/usr/bin/env python
'''log_analyzer.py parses event log generated by AWE and generates performance
results (in tables or figures)
This script is a rather dynamic. And some of the existing functions may be 
used for some specific analysis only. Users may use this as a template to write
new analysis functions for their own needs.'''

import datetime
import json
import matplotlib.pyplot as plt
import numpy as np
import sys
import time
from optparse import OptionParser

color_list = ['b', 'r', 'k', 'g', 'm', 'y']
stage_list = ["prep", "derep", "screen", "fgs", "uclust", "blat"]
cmd_list = ["prep", "drpl", "scrn", "gncl", "clst", "sims", "rsch", "rcls", "rsim", "annt"]  #order matters (MG-RAST production pipeline)

DEFAULT_TOTALWORK = 1
CLIENT_QUOTA = 200

jobperf_dict = {}
taskperf_dict = {}
workperf_dict = {}

def parsePerfLogRaw(filename):
    '''parse perf log'''
    raw_job_dict = {}
    wlf = open(filename, "r")
    
    jobtrace = open(filename + ".jobtrace", "w")
    tasktrace = open(filename + ".tasktrace", "w")
    worktrace = open(filename + ".worktrace", "w")
    
    for line in wlf:
        jobentry = {}
        
        total_data_move = 0
        total_compute = 0
        line = line.strip('\n')
        line = line.strip('\r')
        if len(line)==0:
            continue
        jsonstream = line[26:]
        data =  json.loads(jsonstream)
        linemsg = ""
        for key, val in data.iteritems():
            #print key, val
            if key == 'Ptasks':
                parse_task_perf(val, tasktrace)
            elif key == 'Pworks':
                parse_work_perf(val, worktrace)           
            else:
                if key == "Id":
                    linemsg = "jobid=%s;" % val + linemsg
                    jobentry["jobid"] = val
                else:
                    linemsg += "%s=%s;" % (key.lower(), val)
                    jobentry[str(key.lower())] = val
        linemsg = linemsg[:-1] + "\n"
        jobtrace.write(linemsg)
        jobperf_dict[str(jobentry["jobid"])] = jobentry
                
    print "%d completed jobs have been parsed from the perf log" % len(jobperf_dict.keys())
    
    wlf.close()
    jobtrace.close()
    tasktrace.close()
    worktrace.close()

def parse_task_perf(dataitems, tasktrace):
    for taskid, taskval in dataitems.iteritems():
        task = {}
        msg = "taskid=%s;" % taskid
        parts = taskid.split('_')
        jobid = parts[0]
        msg += "jobid=%s;" % jobid
        stage = parts[1]
        msg += "cmd=%s;" % cmd_list[int(stage)]
        
        task["taskid"] = taskid
        task["jobid"] = jobid
        task["cmd"] = cmd_list[int(stage)]
        
        for k, v in taskval.iteritems():
            if k=="InFileSizes":
                s = sum(v)
                msg += "inputsize=%s;" % s
                task["inputsize"] = s
            elif k=="OutFileSizes":
                s = sum(v)
                msg += "outputsize=%s;" % s
                task["outputsize"] = s
            elif k== "Size":
                pass
            else:
                msg += "%s=%s;" % (k.lower(), v)
                task[str(k.lower())] = v
        msg = msg[:-1] + "\n"
        tasktrace.write(msg)
        taskperf_dict[taskid] = task
    return

def parse_work_perf(dataitems, worktrace):
    for workid, workval in dataitems.iteritems():
        workunit = {}
        parts = workid.split('_')
        jobid = parts[0]
        stage = parts[1]
        rank = parts[2]
        msg = "workid=%s;rank=%s;stage=%s;jobid=%s;cmd=%s;" % (workid, rank, stage, jobid, cmd_list[int(stage)])
        workunit["workid"] = workid
        workunit["rank"] = rank
        workunit["stage"] = stage
        workunit["jobid"] = jobid
        workunit["cmd"] = cmd_list[int(stage)]
        
        for k, v in workval.iteritems():
            msg += "%s=%s;" % (k.lower(), v)
            workunit[str(k.lower())] = v
        msg = msg[:-1] + "\n"
        worktrace.write(msg)
        workperf_dict[workid] = workunit
    return                          

def parseEventLog(filename):
    '''parse event log'''
    raw_job_dict = {}
    wlf = open(filename, "r")
    
    job_dict = {}
    
    for line in wlf:
        line = line.strip('\n')
        line = line.strip('\r')
        if len(line) == 0:
            continue
        if line[0] != "[":
            continue 
        
        timestr = line[1:20]
        
        timeobj = datetime.datetime.strptime(timestr, "%Y/%m/%d %H:%M:%S")
        timestamp = time.mktime(timeobj.timetuple())
        
        infostr = line.split()[4]
        parts = infostr.split(';')
        
        event = parts[0]
        
        attr = {}
        for item in parts[1:]:
            segs = item.split('=')
            key = segs[0]
            val = segs[1]
            attr[key] = val
       
        if event == "JQ":  #job submitted
            job = {}
            id =  attr['jobid']
            job['id'] = id
            job['jid'] = attr['jid']
            job['submit'] = timestamp
            job['total_task'] = 0
            job['task_list'] = {}
            job_dict[id] = job
            
        if event == "TQ" or event == "TD":  # task enqueue
            taskid = attr['taskid']
            segs = taskid.split('_')
            jobid = segs[0]
            stage = int(segs[1])
            if not job_dict.has_key(jobid):
                continue
            
            job = job_dict[jobid]
            anchor = job['submit']
            
            if event == "TQ":
                task_interval = [timestamp-anchor, 0]
                if not job['task_list'].has_key(stage):
                    job['task_list'][stage] = task_interval
            else: #event== "TD"
                if job['task_list'].has_key(stage):
                    job['task_list'][stage][1] = timestamp-anchor
                    job['total_task'] += 1
                
        if event == "JD":
            id =  attr['jobid']
            if not job_dict.has_key(jobid):
                continue
            job_dict[id]['end'] = timestamp
            
        
    for key, value in job_dict.items():
        if not value.has_key('end'):
            del job_dict[key]
     
    print "%d completed jobs have been parsed from the event log" % len(job_dict.keys())    
         
    return job_dict                       


def draw_task_runtime_bar_charts(job_dict):
    '''input job_dict, depict task runtime bar chart for each job'''
    stage_list = ["prep", "derep", "screen", "fgs", "uclust", "blat"]
    for id, job in job_dict.items():
        runtime_list = []
        for item in job['task_list']:
            runtime_list.append(item[1]-item[0])
        draw_taskrun_bar_chart_single(job['jid'], runtime_list, stage_list)
    
def draw_taskrun_bar_chart_single(name, runtime_list, stage_list):
    '''draw task running time bar chart for a single job'''
    N = len(runtime_list)
    ind = np.arange(N)
    
    fig = plt.figure()
    ax = fig.add_subplot(111)
    rects = ax.bar(ind, runtime_list)
    width = 0.35       # the width of the bars

    
    ax.set_ylabel('running time (sec)')
    ax.set_title('running time by each stages')
    ax.set_xticks(ind + width)
    ax.set_xticklabels( stage_list )
    
    def autolabel(rects):
        # attach some text labels
        for rect in rects:
            height = rect.get_height()
            ax.text(rect.get_x()+rect.get_width()/2., 1.0*height, '%d'%int(height),
                    ha='center', va='bottom')
    
    autolabel(rects)
    fig.savefig("%s.png" % name)
    
def draw_task_bars(bins, stage_list, name):

    N = len(stage_list)
    
    ind = np.arange(N)  # the x locations for the groups
    width = 0.15       # the width of the bars
    pad = 0.15

    fig = plt.figure()
    ax = fig.add_subplot(111)
    rects = []
    i = 0
    num_colors = len(color_list)
    for key in bins.keys():
        bin = bins[key]
        rects.append(ax.bar(pad + ind+width*i, bin, width, color=color_list[i % num_colors]))
        i += 1
    
    # add some
    ax.set_ylabel('running time (sec)')
    ax.set_title('running time by each stages')
    ax.set_xticks(pad + ind + width)
    ax.set_xticklabels(stage_list)
    #ax.set_yscale('log')

    #ax.legend( (rects[0][0], rects[1][0]), ('Men', 'Women') )

    fig.savefig("%s.png" % name)
    
def print_task_runtime_table(job_dict):
    for id, job in job_dict.items():
        line = ""
        line += job['jid'] + ","
        for item in job['task_list']:
            runtime = item[1] - item[0]
            line += "%d," % runtime
        line = line[:-1]
        print line
        
def parse_workload(filename):
    
    job_dict = parseEventLog(filename)
    
    wlf = open(filename, "r")
           
    i = 1
    starttime = 0
    
    jobload = []
    taskload = []
    workload = []
    
    jobct = 0
    taskct = 0
    workct = 0    
    
    for line in wlf:
        line = line.strip('\n')
        line = line.strip('\r')
        if len(line) == 0:
            continue
        if line[0] != "[":
            continue 
        
        timestr = line[1:20]
        
        timeobj = datetime.datetime.strptime(timestr, "%Y/%m/%d %H:%M:%S")
        unixtime = int(time.mktime(timeobj.timetuple()))
        
        if i==1:
            starttime = unixtime
        
        timestamp = unixtime - starttime
        
        infostr = line.split()[4]
        parts = infostr.split(';')
        
        event = parts[0]
                        
        attr = {}
        for item in parts[1:]:
            segs = item.split('=')
            key = segs[0]
            val = segs[1]
            attr[key] = val
            
        if event == "JQ":  #job submitted
            jobid = attr["jobid"]
            if job_dict.has_key(jobid):
                jobct += 1
                taskct += job_dict[jobid]["total_task"]
        elif event == "JD":
            if job_dict.has_key(jobid):
                jobct -= 1
        elif event == "TQ":
            taskid = attr['taskid']
            segs = taskid.split('_')
            jobid = segs[0]
            if job_dict.has_key(jobid):
                taskct += 1
                workct += int(attr.get("totalwork", DEFAULT_TOTALWORK))
        elif event == "TD":
            taskid = attr['taskid']
            segs = taskid.split('_')
            jobid = segs[0]
            if job_dict.has_key(jobid):
                taskct -= 1
        elif event == "WD":
            workid = attr['workid']
            segs = workid.split('_')
            jobid = segs[0]
            if job_dict.has_key(jobid):
                workct -= 1
            
        if event in ["JQ", "JD"]:
            jobload.append((timestamp, jobct))
        if event in ["JQ", "TD"]:
            taskload.append((timestamp, taskct))
        if event in ["TQ", "WD"]:
            workload.append((timestamp, workct))
            
        i += 1
        
    return jobload, taskload, workload

def plot_workload(workload, name):
    print "plotting: workload"
    print len(workload), workload[-1]
    fig = plt.figure()
    ax = fig.add_subplot(111)
    plt.title("number of active workunits (queuing + running)")
    interval = 5
    max_point = workload[-1][0] / interval
    timepoint = 0
    timepoints = []
    workct = []
    maxjob = 0
    lastpoint = 0
    for i in range(0, max_point+1):
        timepoint = i * interval
        j = lastpoint
        
        while timepoint >= workload[j][0]:
            j += 1
            if j == len(workload):
                break
        lastpoint = j - 1
        if lastpoint < 0:
            lastpoint = 0
        print timepoint, workload[lastpoint][1], workload[lastpoint]
        workct.append(workload[lastpoint][1])
        timepoints.append(i * interval)
        #print timepoint, workct[i]

    busyclient = []
    for ct in workct:
        if ct >= CLIENT_QUOTA:
            busyclient.append(CLIENT_QUOTA)
        else:
            busyclient.append(ct)
    
    timepoints.append(timepoints[-1] + interval)
    busyclient.append(0)
    workct.append(0)
    
    ax.plot(timepoints, workct, color = "b", lw=1.5)
    ax.plot(timepoints, [CLIENT_QUOTA for i in range(0, len(timepoints))], color = "r")
    ax.fill_between(timepoints, 0, busyclient)
    #ax.set_ylim(0, 160)
    #ax.set_xlim(0, 30000)
    ax.set_xlabel('time elapsed (sec)')
    ax.grid(True)
    print "max_timepoint=", timepoints[-1]
    plt.savefig(name+"-workunit.png")
    
    
def plot_jobload(workload, name):
    print "plotting: workload"
    print len(workload), workload[-1]
    fig = plt.figure()
    ax = fig.add_subplot(111)
    plt.title("number of active jobs (in-progress)")
    interval = 5
    max_point = workload[-1][0] / interval
    timepoint = 0
    timepoints = []
    workct = []
    maxjob = 0
    lastpoint = 0
    for i in range(0, max_point+1):
        timepoint = i * interval
        j = lastpoint
        
        while timepoint >= workload[j][0]:
            j += 1
            if j == len(workload):
                break
        lastpoint = j - 1
        if lastpoint < 0:
            lastpoint = 0
        print timepoint, workload[lastpoint][1], workload[lastpoint]
        workct.append(workload[lastpoint][1])
        timepoints.append(i * interval)
        #print timepoint, workct[i]

    
    timepoints.append(timepoints[-1] + interval)
    workct.append(0)
    
    ax.plot(timepoints, workct, color = "b", lw=1.5)
    
    #ax.set_ylim(0, 160)
    #ax.set_xlim(0, 30000)
    ax.set_xlabel('time elapsed (sec)')
    ax.grid(True)
    print "max_timepoint=", timepoints[-1]
    plt.savefig(name+"-job.png")
    
if __name__ == "__main__":
    p = OptionParser()
    p.add_option("-e", dest = "eventlog", type = "string", 
                    help = "path of event log file")
    
    p.add_option("-p", dest = "perflog", type = "string", 
                    help = "path of perf log file")
    
    p.add_option("-w", dest = "workload", action = "store_true", default = False,
                    help = "draw workload running graph, used with -e")
    
    p.add_option("-r", "--rawjobs", dest = "rawjobs", \
            action = "store_true", \
            default = False, \
            help = "show raw job dict parsed from the event log")
        
    p.add_option("-b", "--bars", dest = "taskbars", \
            action = "store_true", \
            default = False, \
            help = "draw bar chart of task runtime for a list of jobs")
    
    p.add_option("--each", dest = "each", \
            action = "store_true", \
            default = False, \
            help = "draw bar charts of task runtimes for each job")
    
    p.add_option("-t", "--taskcsv", dest = "taskcsv", \
            action = "store_true", \
            default = False, \
            help = "print task runtime .csv, (jobid, task_1_runtime, task_2_runtime, ...)")
    
    (opts, args) = p.parse_args()
    
    if not opts.eventlog and not opts.perflog:
        print "please specify path of either event log file (-e) or perf log file (-p)"
        p.print_help()
        exit()

    
    job_dict = {}
    
    if opts.eventlog:
        job_dict = parseEventLog(opts.eventlog)
    elif opts.perflog:
        parsePerfLogRaw(opts.perflog)
        for k, v in workperf_dict.iteritems():
            print k, v
        for k, v in taskperf_dict.iteritems():
            print k, v
        for k, v in jobperf_dict.iteritems():
            print k, v
        
    if opts.taskbars:
        bins = {}
        for id, job in job_dict.items():
            jid = job['jid']
            bin = []
            for item in job['task_list']:
                runtime = item[1] - item[0]
                bin.append(runtime)
            bins[jid] = bin
        draw_task_bars(bins, stage_list, 'task_runtime')
        
    if opts.each:
        draw_task_runtime_bar_charts(job_dict)
    
    if opts.rawjobs:
        for key, value in job_dict.items():
            print key, value
    
    if opts.taskcsv:
        print_task_runtime_table(job_dict)
        
    if opts.workload:
        if not opts.eventlog:
            print "workload parsing (-w) needs to specify event log (-e)"
            exit()
        jobload, taskload, workload = parse_workload(opts.eventlog)
       
        plot_workload(workload, opts.eventlog.split(".")[0])
        plot_jobload(jobload, opts.eventlog.split(".")[0])
        
       
   
