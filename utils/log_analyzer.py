#!/usr/bin/env python
#log_analyzer.py parses event log generated by AWE and generates performance profile


import time
import datetime
import sys
import matplotlib.pyplot as plt
from optparse import OptionParser

def parseLogFile(filename):
    '''parse the whole work load file'''
    raw_job_dict = {}
    wlf = open(filename, "r")
    
    job_dict = {}
    
    for line in wlf:
        line = line.strip('\n')
        line = line.strip('\r')
        if len(line) == 0:
            continue
        
        timestr = line[1:20]
        
        timeobj = datetime.datetime.strptime(timestr, "%Y/%m/%d %H:%M:%S")
        timestamp = time.mktime(timeobj.timetuple())
        
        infostr = line.split()[4]
        parts = infostr.split(';')
        
        event = parts[0]
        
        attr = {}
        for item in parts[1:]:
            segs = item.split('=')
            key = segs[0]
            val = segs[1]
            attr[key] = val
       
        if event == "JQ":  #job submitted
            job = {}
            id =  attr['jobid']
            job['id'] = id
            job['jid'] = attr['jid']
            job['submit'] = timestamp
            job['total_task'] = 0
            job['task_list'] = []
            job_dict[id] = job
            
        if event == "TQ" or event == "TD":  # task enqueue
            taskid = attr['taskid']
            segs = taskid.split('_')
            jobid = segs[0]
            stage = int(segs[1])
            if not job_dict.has_key(jobid):
                continue
            
            job = job_dict[jobid]
            anchor = job['submit']
            
            if event == "TQ":
                task_interval = [timestamp-anchor, 0]
                if stage == job['total_task']: #record the first TQ
                    job['task_list'].append(task_interval) 
                    job['total_task'] += 1
                else:
                    del job_dict[jobid]
                    continue   # ignore jobs that have task re-queued for now
            else:
                job['task_list'][-1][1] = timestamp-anchor
                
                
        if event == "JD":
            id =  attr['jobid']
            if not job_dict.has_key(jobid):
                continue
            job_dict[id]['end'] = timestamp
            
        
    for key, value in job_dict.items():
        if not value.has_key('end'):
            del job_dict[key]
        else:
            value['time_points'] = [0]
            anchor = job['submit']
            for item in job['task_list']:
                timepoint = item[0] - anchor              
     
    for key, value in job_dict.items():
        print key, value     
     
    return job_dict                       
    #return job_dict, min_qtime, min_start, max_qtime, max_end

def get_task_runtime(job_dict):
    '''input job_dict, return the task runtime for each job'''
    pass

if __name__ == "__main__":
    p = OptionParser()
    p.add_option("-l", dest = "logfile", type = "string", 
                    help = "path of log file (required)")
    p.add_option("-a", "--alloc", dest = "alloc", \
            action = "store_true", \
            default = False, \
            help="plot bars represent for individual jobs ")
  
    (opts, args) = p.parse_args()
    
    if not opts.logfile:
        print "please specify path of log file"
        p.print_help()
        exit()

    job_dict = parseLogFile(opts.logfile)
      
    